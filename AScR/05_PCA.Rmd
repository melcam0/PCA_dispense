---
title: "Analisi delle componenti principali"
date: ''
link-citations: yes
header-includes:
 - \hypersetup{backref,colorlinks=true}
output:
  pdf_document:
    keep_tex: no
  html_document:
    df_print: paged
bibliography: Biblio.bib
---




## Dati multidimensionali

### Rappresentazione matriciale e geometrica

  
 
\begin{table}[h]
\caption{Rappresentazione matriciale}
\label{tab:RegrMult}
\ 
\begin{center}
\begin{tabular}{rrrrrr}
\hline
Indiv. & $X_1$ & $X_2$ & $\dots$ & $X_p$ \\
\hline
1 & $x_{11}$ & $x_{12}$ & $\dots$ & $x_{1p}$\\
2 & $x_{21}$ & $x_{22}$ & $\dots$ & $x_{2p}$ \\
...\\
m & $x_{m1}$ & $x_{m2}$ & $\dots$ & $x_{mp}$ \\
\hline
\end{tabular}
\end{center}
\end{table}
 

```{r,fig.width=8, fig.height=4, echo=FALSE}
par(mfrow=c(1,2))
plot(iris[1,1:2],fg="white",col.lab="white",col.axis="white",xlim=c(-1,10),ylim=c(-1,6)
     ,asp=1)
segments(x0 = 5.1,y0 = 0,x1 = 5.1,y1 = 3.5,lty = 2)
text(x = 5.1,y = -0.5,labels = expression(x[paste("i",",","1")]),cex=0.8)
segments(x0 = 0,y0 = 3.5,x1 = 5.1,y1 = 3.5,lty = 2)
text(x = -0.5,y = 3.5,labels = expression(x[paste("i",",","2")]),cex=0.8)
arrows(x0 = 0,y0 = 0,x1 = 10,y1 = 0,length = 0.20, angle = 15,lwd=1)
text(x = 10,y = -0.5,labels=expression(X[1]),font=1)
arrows(x0 = 0,y0 = 0,x1 = 0,y1 = 5,length = 0.20, angle = 15,lwd=1)
text(x = -0.5,y = 5,labels=expression(X[2]),font=1)
text(x = 0,y = -0.5,labels=expression(0),font=1)
text(x = 7,y = 3.5,labels = "individuo i")

plot(iris[,1:2],fg="white",col.lab="white",col.axis="white",xlim=c(-1,10),ylim=c(-1,6),
     asp=1,cex=0.5)
arrows(x0 = 0,y0 = 0,x1 = 10,y1 = 0,length = 0.20, angle = 15,lwd=1)
text(x = 10,y = -0.5,labels=expression(X[1]),font=1)
arrows(x0 = 0,y0 = 0,x1 = 0,y1 = 5,length = 0.20, angle = 15,lwd=1)
text(x = -0.5,y = 5,labels=expression(X[2]),font=1)
text(x = 0,y = -0.5,labels=expression(0),font=1)
```





### Trasformazione delle variabili: centratura e standardizzazione
 
Indichiamo con $\bar{x_1},\dots,\bar{x_p}$ le medie delle variabili $X_1,\dots,X_p$,
cioè le $p$ medie delle $p$ colonne della Tabella \ref{tab:RegrMult}, e con
$\sigma_1^2,\dots,\sigma_p^2$ le rispettive varianze.  
Il vettore $\bar{x}=(\bar{x_1},\dots,\bar{x_p})$ viene chiamato **baricentro**.
 
**Centratura**: semplice traslazione del baricentro nell'origine
\begin{equation}
x_{ij}^{'}=x_{ij}-\bar{x_j}
\end{equation}
  
  - non perdo informazione sulla distanza tra i punti (la geometria della nuvola di punti
   rimane invariata)
  - perdo solo informazione sul baricentro
  - semplifica formule e conti (da ora in poi useremo sempre dati centrati)
  
**Standardizzazione**: questa trasformazione porta ogni variabile ad avere varianza $1$
(in generale questa trasformazione viene fatta insieme alla centratura)
\begin{equation}
x_{ij}^{'}=\frac{x_{ij}-\bar{x_j}}{\sigma_j}
\end{equation}
  
 - questa trasformazione rende le variabili degli scalari (numeri puri)
 - questa trasformazione è necessaria quando si vogliono confrontare variabili
 con differenti unità di misura (le variabili devono essere omogenee per essere confrontabili)
 - tutte le variabili hanno lo stesso "peso"
 - cambia la distanza (la geometria) tra i punti. E' una dilatazione o contrazione.
     
Si veda la seguente figura per una rappresentazione grafica di dati centarti e
scalati per una matrice di dati di $2$ variabili

```{r,fig.width=12, fig.height=11, echo=FALSE}
layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))

plot(iris[,1:2],fg="white",col.lab="white",col.axis="white",xlim=c(-1,10),ylim=c(-1,6),
     asp=1)
arrows(x0 = 0,y0 = 0,x1 = 10,y1 = 0,length = 0.20, angle = 15,lwd=1)
text(x = 10,y = -0.5,labels=expression(X[1]),font=1)
arrows(x0 = 0,y0 = 0,x1 = 0,y1 = 5,length = 0.20, angle = 15,lwd=1)
text(x = -0.5,y = 5,labels=expression(X[2]),font=1)
text(x = 0,y = -0.5,labels=expression(0),font=1)
text(x=5,y=-1,labels="Dati originali")

plot(scale(iris[,1:2],center = TRUE,scale = FALSE),fg="white",col.lab="white",
     col.axis="white",xlim=c(-3.5,3),ylim=c(-1,2),asp=1)
arrows(x0 = -2.5,y0 = 0,x1 = 3,y1 = 0,length = 0.20, angle = 15,lwd=1)
text(x = 3,y = -0.5,labels=expression(X[1]),font=1)
arrows(x0 = 0,y0 = -1.5,x1 = 0,y1 = 2,length = 0.20, angle = 15,lwd=1)
text(x = -0.5,y = 2,labels=expression(X[2]),font=1)
text(x=1.5,y=-2,labels="Dati centrati")

plot(scale(iris[,1:2],center = TRUE,scale = TRUE),fg="white",col.lab="white",
     col.axis="white",xlim=c(-3.5,3),ylim=c(-1,2),asp=1)
arrows(x0 = -2.5,y0 = 0,x1 = 3,y1 = 0,length = 0.20, angle = 15,lwd=1)
text(x = 3,y = -0.5,labels=expression(X[1]),font=1)
arrows(x0 = 0,y0 = -1.5,x1 = 0,y1 = 2,length = 0.20, angle = 15,lwd=1)
text(x = -0.5,y = 2,labels=expression(X[2]),font=1)
text(x=2,y=-2,labels="Dati centrati e scalati")
```












### Matrice di covarianza e correlazione
 
 
 
 
 
\begin{equation}\label{eq:Corr}
Cov(X)=\left(
\begin{array}{cccc}
\sigma_{11}  & \dots & \sigma_{1p} \\
\vdots & \quad & \vdots \\
\sigma_{m1} & \dots & \sigma_{pp} \\
\end{array}
\right),
\end{equation}
dove $\sigma_{ij}=\frac{1}{m-1}\sum_{k=1}^m(x_{ki}-\bar{x_i})(x_{kj}-\bar{x_j})$ è la covarianza tra
le variabili $X_i$ e $X_j$,
e in particolare   
$\sigma_{ii}=\sigma_i^2=\frac{1}{m-1}\sum_{k=1}^m(x_{ki}-\bar{x_i})^2$ è la varianza della variabile
$X_i$.  
Nel caso in cui i dati siano centrati $Cov(X)=\frac{1}{m-1}X^tX$
 
 
 
 
\begin{equation}\label{eq:Corr}
Cor(X)=\left(
\begin{array}{cccc}
1  & \dots & r_{1p} \\
\vdots & \quad & \vdots \\
r_{m1} & \dots & 1\\
\end{array}
\right),
\end{equation}
dove $r_{ij}=\frac{\sigma_{ij}}{\sqrt{\sigma_{ii}\sigma_{jj}}}$ è la correlazione tra
le variabili $X_i$ e $X_j$.
   
   
 

```{r,fig.width=8, fig.height=4, echo=FALSE}
library(mvtnorm)
set.seed(12345)
sigma=matrix(c(5,0,0,1),ncol=2)
X=rmvnorm(200,mean=c(0,0),sigma=sigma)

par(mfrow=c(1,2))

plot(X,fg="white",col.lab="white",col.axis="white",xlim=c(-8,8),ylim=c(-5,5),
     asp=1,cex=0.5)
arrows(x0 = -8,y0 = 0,x1 = 8,y1 = 0,length = 0.20, angle = 15,lwd=1)
text(x = 8,y = -0.5,labels=expression(X[1]),font=1)
arrows(x0 = 0,y0 = -5,x1 = 0,y1 = 5,length = 0.20, angle = 15,lwd=1)
text(x = -0.5,y = 5,labels=expression(X[2]),font=1)
text(x = 0,y = -0.5,labels=expression(0),font=1)
text(x=-5,y=-4.5,labels = expression(Cov(X)==bgroup("(",paste(atop(5,0)," ",atop(0,1)),")")),
     cex=0.7)
text(x=5,y=-4.5,labels = expression(Cor(X)==bgroup("(",paste(atop(1,0)," ",atop(0,1)),")")),
     cex=0.7)


plot(scale(X),fg="white",col.lab="white",col.axis="white",xlim=c(-8,8),ylim=c(-5,5),
     asp=1,cex=0.5)
arrows(x0 = -8,y0 = 0,x1 = 8,y1 = 0,length = 0.20, angle = 15,lwd=1)
text(x = 8,y = -0.5,labels=expression(X[1]),font=1)
arrows(x0 = 0,y0 = -5,x1 = 0,y1 = 5,length = 0.20, angle = 15,lwd=1)
text(x = -0.5,y = 5,labels=expression(X[2]),font=1)
text(x = 0,y = -0.5,labels=expression(0),font=1)
text(x=-5,y=-4.5,labels = expression(Cov(X)==bgroup("(",paste(atop(1,0)," ",atop(0,1)),")")),
     cex=0.7)
text(x=5,y=-4.5,labels = expression(Cor(X)==bgroup("(",paste(atop(1,0)," ",atop(0,1)),")")),
     cex=0.7)
```




```{r,fig.width=8, fig.height=4, echo=FALSE}
sigma=matrix(c(5,1.79,1.79,1),ncol=2)
X=rmvnorm(200,mean=c(0,0),sigma=sigma)

par(mfrow=c(1,2))

plot(X,fg="white",col.lab="white",col.axis="white",xlim=c(-8,8),ylim=c(-5,5),
     asp=1)
arrows(x0 = -8,y0 = 0,x1 = 8,y1 = 0,length = 0.20, angle = 15,lwd=1)
text(x = 8,y = -0.5,labels=expression(X[1]),font=1)
arrows(x0 = 0,y0 = -5,x1 = 0,y1 = 5,length = 0.20, angle = 15,lwd=1)
text(x = -0.5,y = 5,labels=expression(X[2]),font=1)
text(x = 0,y = -0.5,labels=expression(0),font=1)
text(x=5,y=-4.5,labels = expression(Cor(X)==bgroup("(",paste(atop(1,0.8)," ",atop(0.8,1)),")")),
     cex=0.7)
text(x=-5,y=-4.5,labels = expression(Cov(X)==bgroup("(",paste(atop(5,1.79)," ",atop(1.79,1)),")")),
     cex=0.7)



plot(scale(X),fg="white",col.lab="white",col.axis="white",xlim=c(-8,8),ylim=c(-5,5),
     asp=1)
arrows(x0 = -8,y0 = 0,x1 = 8,y1 = 0,length = 0.20, angle = 15,lwd=1)
text(x = 8,y = -0.5,labels=expression(X[1]),font=1)
arrows(x0 = 0,y0 = -5,x1 = 0,y1 = 5,length = 0.20, angle = 15,lwd=1)
text(x = -0.5,y = 5,labels=expression(X[2]),font=1)
text(x = 0,y = -0.5,labels=expression(0),font=1)
text(x=-5,y=-4.5,labels = expression(Cov(X)==bgroup("(",paste(atop(1,0.8)," ",atop(0.8,1)),")")),
     cex=0.7)
text(x=5,y=-4.5,labels = expression(Cor(X)==bgroup("(",paste(atop(1,0.8)," ",atop(0.8,1)),")")),
     cex=0.7)
```




### Variabili latenti o componenti e proiezioni \label{par:VariabiliLatenti}
 
Sia $T$ la combinazione lineare delle variabili $X_1,\dots,X_p$, ossia il vettore
(si veda Figura \ref{fig:versore})
\begin{equation}
T=b_1X_1+\dots+b_pX_p,
\end{equation}
dove $b_1^2+\dots+b_p^2=1$. Il vettore $\bold{b}=(b_1,\dots,b_p)$ è chiamato versore
e indica la direzione della variabile latente $T$ (si veda Figura \ref{fig:versore}).


```{r,fig.width=5, fig.height=4,fig.cap="Variabile latente $T$\\label{fig:versore}" ,echo=FALSE}
plot(NULL,fg="white",col.lab="white",col.axis="white",xlim=c(-1,10),ylim=c(-1,6),
     asp=1)
arrows(x0 = 0,y0 = 0,x1 = 10,y1 = 0,length = 0.20, angle = 15,lwd=1)
text(x = 10,y = -0.5,labels=expression(X[1]),font=1)
arrows(x0 = 0,y0 = 0,x1 = 0,y1 = 5,length = 0.20, angle = 15,lwd=1)
text(x = -0.5,y = 5,labels=expression(X[2]),font=1)
text(x = 0,y = -0.5,labels=expression(0),font=1)
abline(a = 0,b = 1)
arrows(x0 = 0,y0 = 0,x1 = 1/sqrt(2),y1 = 1/sqrt(2),length = 0.15, angle = 10,lwd=2,
       col="blue")
text(x = 1.7,y =  1/sqrt(2)-0.1,labels=expression(group("(",list(b[1],b[2]),")")),font=1,
     cex=0.8,col="blue")
text(x = 7,y = 5,labels = expression(T==b[1]*X[1]+b[2]*X[2]),cex=0.8)

```


   
Sia $\bold{x}=(x_1,\dots,x_p)$ un generico punto (vettore) di $\mathbf{R^p}$.
Chiamiamo proiezione di $\bold{x}$ su $T$ il punto $\bold{x'}$ di $T$ la 
cui distanza da $\bold{x}$ è minima (si veda Figura \ref{fig:proiezione})


```{r,fig.width=5, fig.height=4,fig.cap="Proiezione su $T$\\label{fig:proiezione}",echo=FALSE}
plot(x=2,y=4,fg="white",col.lab="white",col.axis="white",xlim=c(-1,10),ylim=c(-1,6),
     asp=1)
arrows(x0 = 0,y0 = 0,x1 = 10,y1 = 0,length = 0.20, angle = 15,lwd=1)
text(x = 10,y = -0.5,labels=expression(X[1]),font=1)
arrows(x0 = 0,y0 = 0,x1 = 0,y1 = 5,length = 0.20, angle = 15,lwd=1)
text(x = -0.5,y = 5,labels=expression(X[2]),font=1)
text(x = 0,y = -0.5,labels=expression(0),font=1)
abline(a = 0,b = 1)
arrows(x0 = 0,y0 = 0,x1 = 1/sqrt(2),y1 = 1/sqrt(2),length = 0.15, angle = 10,lwd=2,
       col="blue")
text(x = 1.7,y =  1/sqrt(2)-0.1,labels=expression(group("(",list(b[1],b[2]),")")),font=1,
     cex=0.8,col="blue")
text(x = 7,y = 5,labels = expression(T==b[1]*X[1]+b[2]*X[2]),cex=0.8)
text(x = 1.5,y = 4.5,labels = "x",cex=0.8,font=2)
text(x = 3.5,y = 2.5,labels = "x'",cex=0.8,font=2)
segments(x0 = 3,y0 = 3,x1 = 2,y1 = 4,lty = 2)
par(new=TRUE)
plot(x=3,y=3,fg="white",col.lab="white",col.axis="white",xlim=c(-1,10),ylim=c(-1,6),asp=1,
     pch=20,col="red")
```




Definiamo *componente* di **x** su T la lunghezza del vettore $\|\bold{x'} \|$ data da
\begin{equation}
\|\bold{x'}\|=b_1x_1+\dots+b_px_p.
\end{equation}
I valori $b_1,\dots,b_p$ sono chiamati *loading*  e la quantità
$b_1x_1+\dots+b_px_p$ *score*.  
 
Si osservi che 
\begin{equation}
\| \bold{x'} \|=\|\bold{x} \|\cos \theta
\end{equation}
ossia al prodotto interno (scalare) tra i vettori $\bold{x}$ e $\bold{b}$ ($\| \bold{b}\|=1$).
Si veda la Figura \ref{fig:prodinterno}.

```{r,fig.width=5, fig.height=4,echo=FALSE, fig.cap="Prodotto interno tra **x** e **b** \\label{fig:prodinterno}"}
plot(x=2,y=4,fg="white",col.lab="white",col.axis="white",xlim=c(-1,10),ylim=c(-1,6),
     asp=1)
text(x = 0,y = -0.5,labels=expression(0),font=1)
abline(a = 0,b = 1)
arrows(x0 = 0,y0 = 0,x1 = 1/sqrt(2),y1 = 1/sqrt(2),length = 0.15, angle = 10,lwd=2,
       col="blue")
text(x = 1.5,y =  1/sqrt(2)-0.1,labels="b",font=2,
     cex=0.8,col="blue")
text(x = 1.5,y = 4.5,labels = "x",cex=0.8,font=2)
text(x = 3.5,y = 2.5,labels = "x'",cex=0.8,font=2)
segments(x0 = 3,y0 = 3,x1 = 2,y1 = 4,lty = 1)
par(new=TRUE)
plot(x=3,y=3,fg="white",col.lab="white",col.axis="white",xlim=c(-1,10),ylim=c(-1,6),asp=1,
     pch=20,col="red")
arrows(x0 = 0,y0 = 0,x1 = 2,y1 = 4,length = 0.15, angle = 10,lwd=2,
       col="blue")
text(x = 1,y =1.3 ,labels = expression(theta),cex=0.8)
```



Proiezione degli $m$ individui della matrice **X** sulla variabile latente T
\begin{equation}
\left(
\begin{array}{cccc}
x_{11}  & \dots & x_{1p} \\
\vdots & \quad & \vdots \\
x_{m1} & \dots & x_{mp} \\
\end{array}
\right)
\left(
\begin{array}{c}
b_1 \\
\vdots \\
b_m \\
\end{array}
\right)
=
\left(
\begin{array}{cccc}
b_1x_{11}  + \dots +b_p x_{1p} \\
\vdots   \\
b_1x_{m1} + \dots + b_px_{mp} \\
\end{array}
\right).
\end{equation}
   
Supponiamo di prendere una seconda variabile latente
\begin{equation}
T'=b'_1X_1+\dots+b'_pX_p, \qquad (b'_1)^2+\dots+(b'_p)^2=1
\end{equation}
e supponiamo che sia ortogonale a T (i.e **b** e **b'** ortogonali)
\begin{equation}
b_1b'_1+\dots+b_pb'_p=0.
\end{equation}
Si veda la Figura \ref{fig:proiezpiano}.


```{r,fig.width=5, fig.height=4,echo=FALSE, fig.cap="Proiezione sul piano TT' \\label{fig:proiezpiano}"}
plot(x=2,y=4,fg="white",col.lab="white",col.axis="white",xlim=c(-2,10),ylim=c(-1,6),
     asp=1)
arrows(x0 = -5,y0 = 0,x1 = 10,y1 = 0,length = 0.20, angle = 15,lwd=1)
text(x = 10,y = -0.5,labels=expression(X[1]),font=1)
arrows(x0 = 0,y0 = -3,x1 = 0,y1 = 5,length = 0.20, angle = 15,lwd=1)
text(x = -0.5,y = 5,labels=expression(X[2]),font=1)
#text(x = 0,y = -0.5,labels=expression(0),font=1)
abline(a = 0,b = 1,col="blue")
arrows(x0 = 0,y0 = 0,x1 = 1/sqrt(2),y1 = 1/sqrt(2),length = 0.15, angle = 10,lwd=2,
       col="blue")
text(x = 1.7,y =  1/sqrt(2)-0.1,labels=expression(group("(",list(b[1],b[2]),")")),font=1,
     cex=0.8,col="blue")
abline(a = 0,b = -1, col="blue")
arrows(x0 = 0,y0 = 0,x1 = -1/sqrt(2),y1 = 1/sqrt(2),length = 0.15, angle = 10,lwd=2,
       col="blue")
text(x =- 1.7,y =  1/sqrt(2)-0.1,labels=expression(group("(",list("b'"[1],"b'"[2]),")")),font=1,
     cex=0.8,col="blue")
#text(x = 7,y = 5,labels = expression(T==b[1]*X[1]+b[2]*X[2]),cex=0.8)
text(x = 1.5,y = 4.5,labels = "x",cex=0.8,font=2)
text(x = 3.5,y = 2.5,labels = "x'",cex=0.8,font=2)
text(x = -1,y = 1.5,labels = "x''",cex=0.8,font=2)
segments(x0 = 3,y0 = 3,x1 = 2,y1 = 4,lty = 2)
segments(x0 = -1,y0 = 1,x1 = 2,y1 = 4,lty = 2)
par(new=TRUE)
plot(x=3,y=3,fg="white",col.lab="white",col.axis="white",xlim=c(-2,10),ylim=c(-1,6),asp=1,
     pch=20,col="red")
par(new=TRUE)
plot(x=-1,y=1,fg="white",col.lab="white",col.axis="white",xlim=c(-2,10),ylim=c(-1,6),asp=1,
     pch=20,col="red")
```


   
Proiezione degli $m$ individui della matrice **X** sul piano TT'
\begin{equation}
\left(
\begin{array}{cccc}
x_{11}  & \dots & x_{1p} \\
\vdots & \quad & \vdots \\
x_{m1} & \dots & x_{mp} \\
\end{array}
\right)
\left(
\begin{array}{cc}
b_1 & b'_1\\
\vdots & \vdots \\
b_m & b'_p\\
\end{array}
\right)
=
\left(
\begin{array}{cccc}
b_1x_{11}  + \dots +b_p x_{1p} & b'_1x_{11}  + \dots +b'_p x_{1p}\\
\vdots  & \vdots \\
b_1x_{m1} + \dots + b_px_{mp} & b'_1x_{m1} + \dots + b'_px_{mp} \\
\end{array}
\right).
\end{equation}
   
   
E' possibile iterare questo procedimento fino a p variabili latenti, in questo caso otteniamo
un cambio di basi (nuove coordinate). Abbiamo semplicemente "cambiato prospettiva" ruotando
il sistema di coordinate. Si veda la Figura \ref{fig:cambiobase}.
 
 
E' possibile fermarsi prima e proiettare su un 
iperpiano,
   
Questo procedimento viene in generale eseguito perchè le variabili latenti
hanno certe proprietà desiderate.
 
   
Indicando con
\begin{equation}
P=
 \left(
 \begin{array}{cccc}
 b^1_1  & b^2_1 & \dots & b^p_1 \\
 \vdots &\vdots & \quad & \vdots \\
 b^2_m & b^2_m & \dots & b^p_m \\
 \end{array}
 \right) 
\end{equation}
la matrice dei *loading*, si ha
\begin{equation}
 T=XP
\end{equation}
e ricordando l'ortonormalità dei vettori $\bold{b}_1,\dots,\bold{b}_p$ ($P^tP=I$)
\begin{equation}
 X=TP^t
\end{equation}


```{r,fig.width=9, fig.height=4,echo=FALSE, fig.cap="Cambio base da $X_1X_2$ a $T_1T_2$ \\label{fig:cambiobase}"}
library(mvtnorm)
sigma=matrix(c(1,0.8,0.8,1),ncol=2)
set.seed(1234)
X=rmvnorm(200,mean=c(0,0),sigma=sigma)

par(mfrow=c(1,2))

plot(X,fg="white",col.lab="white",col.axis="white",xlim=c(-6.5,6),ylim=c(-4,4),
     asp=1,col="grey30")
arrows(x0 = -5.5,y0 = 0,x1 = 6 ,y1 = 0,length = 0.20, angle = 15,lwd=1)
text(x = 6,y = -0.5,labels=expression(X[1]),font=1)
arrows(x0 = 0,y0 = -4,x1 = 0,y1 = 4,length = 0.20, angle = 15,lwd=1)
text(x = -0.5,y = 4,labels=expression(X[2]),font=1)
#text(x = 0,y = -0.5,labels=expression(0),font=1)
abline(a = 0,b = 1,col="blue",lty=2)
arrows(x0 = 0,y0 = 0,x1 = 1/sqrt(2),y1 = 1/sqrt(2),length = 0.15, angle = 10,lwd=2,
       col="blue")
text(x = 1.7,y =  1/sqrt(2)-0.1,labels=expression(b^1),font=2,
     cex=0.8,col="blue")
abline(a = 0,b = -1, col="blue",lty=2)
arrows(x0 = 0,y0 = 0,x1 = -1/sqrt(2),y1 = 1/sqrt(2),length = 0.15, angle = 10,lwd=2,
       col="blue")
text(x =- 1.7,y =  1/sqrt(2)-0.1,labels=expression(b^2),font=2,
     cex=0.8,col="blue")
text(x = 5,y = 3,labels = expression(T[1]),col="blue")
text(x = 5,y = 3,labels = expression(T[1]),col="blue")
text(x = -5.5,y = 3,labels = expression(T[2]),col="blue")
text(x=4,y=-1.5,labels = expression(Cov(X)==bgroup("(",paste(atop(1,0.8)," ",atop(0.8,1)),")")),
     cex=0.7)

P=matrix(c(1/sqrt(2),1/sqrt(2),-1/sqrt(2),1/sqrt(2)),ncol=2)
T=X%*%P

plot(T,fg="white",col.lab="white",col.axis="white",xlim=c(-6.5,6),ylim=c(-4,4),
     asp=1,col="grey30")
arrows(x0 = -5.5,y0 = 0,x1 = 6 ,y1 = 0,length = 0.20, angle = 15,lwd=1)
text(x = 6,y = -0.5,labels=expression(T[1]),font=1)
arrows(x0 = 0,y0 = -4,x1 = 0,y1 = 4,length = 0.20, angle = 15,lwd=1)
text(x = -0.5,y = 4,labels=expression(T[2]),font=1)
text(x=-4,y=-3,labels = expression(Cov(T)==bgroup("(",paste(atop(2.03,0.01),"  ",atop(0.01,0.18)),")")),
     cex=0.7)
text(x=4,y=-3,labels = expression(Cor(T)==bgroup("(",paste(atop(1,0.02),"  ",atop(0.02,1)),")")),
     cex=0.7)
```






```{r}
P=matrix(c(1/sqrt(2),1/sqrt(2),-1/sqrt(2),1/sqrt(2)),ncol=2)
T=X%*%P
head(T)
```



## Analisi delle componenti principali
 
Vogliamo costruire le variabili latenti $T_1,\dots,T_p$ in modo da massimalizzare la distanza 
tra gli $m$ oggetti in $\mathbb{R}^p$, le cui coordinate sono date dalla matrice $X$ (cf. ), 
nel senso che punti lontani in $\mathbb{R}^p$ siano il più
lontano possibile nelle proiezioni su $T_1$, poi $T_2$,...... 
La distanza tra i punti può essere misurata usando il teorema di Pitagora, distanza euclidea,
e questa è la formula della varianza delle variabili $X_1,\dots,X_p$.     
Vogliamo massimalizzare la varianza, perchè ad essa è associata l'informazione contenuta nei dati
in esame.
In definitiva vogliamo massimalizzare l'informazione ricavabile dagli oggetti in esama (varianza).
    
E' posssibile determinare una variabile latente $T_1$, che chiameremo *Prima Componente Principale*,
in  modo tale che
 
\begin{equation}
Var (T_1)=\rm{Max}_{\it{T}}\it{Var(T)}
\end{equation}
al variare di tutte le direzioni possibili $T$ in $\mathbb{R}^p$.  
Tra tutte le variabili latenti perpendicolari alla $T_1$ è possibile determinare una seconda
variabile latente $T_2$, che chiameremo *Seconda Componente Principale* in modo tale che
 
\begin{equation}
Var (T_2)=\rm{Max}_{\it{T\perp T_1}}\it{Var(T)}
\end{equation}
 
Questo procedimento può essere iterato fino alla costruzione di $p$  componenti principali
$T_1,T_2,\dots,T_p$.
 
Per quanto visto nel Paragrafo \ref{par:VariabiliLatenti} abbiamo determinato la matrice $P$ 
dei *loading*. La matrice degli *score* si ottiene 
\begin{equation}\label{eq:Score}
T=XP
\end{equation}
 
La procedura per determinare $P$ passa attraverso il calcolo degli autovalori 
$\lambda_1,\dots,\lambda_p$ della matrice di covarianza (di correlazione nel caso in cui i dati
fossero stati standardizzati)
\begin{equation}
Cov(X)=X^tX
\end{equation}
 
e dei relativi autovettori (le $p$ componenti principali).  
   
Uno dei risultati principali di questa costruzione è che nel sistema di coordinate delle componenti principali
 
\begin{equation}\label{eq:Corr}
Cov(T)=\left(
\begin{array}{cccc}
\lambda_1  & \dots & 0 \\
\vdots & \quad & \vdots \\
0 & \dots & \lambda_m \\
\end{array}
\right),
\quad \lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_m
\end{equation}
 
Conseguenze  
 
* $Var(T_i)=\lambda_i$   
* varianza totale: $\lambda_1+\dots+\lambda_p$  
* le componenti $T_1,T_2,\dots,T_p$ sono a indipendenti
 
 
 
 ################################
 
 
### Esempio iris
 
Consideriamo il dataset *iris*
```{r}
head(iris)
```


formato da $150$ fiori appartenenti a $3$ specie della pianta iris
```{r}
summary(iris[,5])
```


misurati secondo le variabili
```{r}
names(iris[,1:4])
```


   
L'analisi delle Componenti Principali si conduce usando la funzione *PCA* del 
pacchetto *FactoMineR*
```{r}
library(FactoMineR)
iris.pca=PCA(X = iris,scale.unit = TRUE,quali.sup = 5,graph = FALSE,ncp = 4)
```


Si noti che abbiamo usato l'opzione *scale.unit=TRUE*.  
L'oggetto *iris.pca* è composto dalle seguenti famiglie di oggetti
```{r}
names(iris.pca)
```

#### Autovalori/varianza
Nell'oggetto *iris.pca$eig* abbiamo i seguenti oggetti
```{r}
names(iris.pca$eig)
iris.pca$eig
```



```{r,fig.width=5, fig.height=4,echo=FALSE, fig.cap="Autovalori \\label{fig:autoval}"}
p=barplot(height = iris.pca$eig[,1],xlab = "Componenti Principali",
        ylim = c(0,iris.pca$eig[1,1]+0.3),
        ylab = "Percentuale varianza spiegata",names.arg = 1:length(iris.pca$eig[,1]),
        col = "steelblue")
lines(x = p, iris.pca$eig[,1], 
      type = "b", pch = 19, col = "red")
for (i in 1:length(iris.pca$eig[,1])){
  text(x = p[i]+0.2,y = iris.pca$eig[i,1]+0.2,col="red",cex=0.7,
       labels =paste(round(iris.pca$eig[i,2],1), "%" ))
}
```





#### Individui
L'algoritmo utilizzato dal pacchetto *FactoMineR* è la *Decomposizione ai
valori singolari* della matrice $X$, ossia la matrice $X$ viene decomposta 
nel prodotto di $3$ matrici
\begin{equation}
X=T_0.Diag(vs).P^t,
\end{equation}
dove $T_0$ è la matrice degli *score* normalizzati di lunghezza $1$, $Diag(vs)$ è
una matrice diagonale i cui valori non nulli (*valori singolari*) sono la radice
degli autovalori $\lambda_1,\dots,\lambda_p$ (cf. (\ref{eq:Corr})).  
La matrice degli *score* $T$ si ottiene
\begin{equation}
T=T_0.Diag(vs).
\end{equation}
    
Le matrici $T_0, Diag(vs)$ e $P$ sono memorizzate nell'oggetto *iris.pca$svd*
```{r}
names(iris.pca$svd)
```


 
```{r}
T0=iris.pca$svd$U
L=diag(iris.pca$svd$vs)
P=iris.pca$svd$V
```



L'oggetto *iris.pca$ind* contiene i seguenti oggetti
```{r}
names(iris.pca$ind)
head(iris.pca$ind$coord)
head(T0%*%L) 
```




```{r,fig.width=5, fig.height=4,echo=FALSE, fig.cap="Individui \\label{fig:individui}"}
plot(x = iris.pca$ind$coord[,1],y = iris.pca$ind$coord[,2],asp = 1,pch=20,
     col=c("black","red","green")[iris$Species],
     xlab=paste("Comp. 1","(",round(iris.pca$eig[1,2],2),"%",")"),
     ylab=paste("Comp. 2","(",round(iris.pca$eig[2,2],2),"%",")"))
legend("topleft",legend = levels(iris$Species),text.col=c("black","red","green"),cex=0.7)
abline(h = 0,lty=2)
abline(v = 0,lty=2)
```



```{r}
head(iris.pca$ind$cos2) # qualità della rappresentazione (vedi fig)

X=iris.pca$ind$cos2[,1]+iris.pca$ind$cos2[,2]
head(X[order(X,decreasing = TRUE)])
tail(X[order(X,decreasing = TRUE)])
```





#### Variabili
 
Un'altra importamte caratteristica dell'Analisi delle Componenti Principali
è che possibile "rappresenatare nel nuovo sitema di coordinate" le $p$ 
variabili $X_1,\dots,X_p$.  
Dalla (\ref{eq:Score}), ricordando che $T^tT=I$
\begin{equation}
X=TP^t.
\end{equation}
Indicando (cf. Paragrafo \ref{par:VariabiliLatenti})
\begin{equation}
P=(\textbf{b}^1,\dots,\textbf{b}^p)=
\left(
\begin{array}{cccc}
b_1^1  & \dots & b_1^p\\
\vdots & \quad & \vdots \\
b_p^1 & \dots & b_p^p \\
\end{array}
\right)
\end{equation}
si ottiene
\begin{equation}
X_i=b_i^1T_1+\dots+b_i^pT_p.
\end{equation}
Ricordando la (\ref{eq:Corr}) si ottiene facilmente
\begin{eqnarray}
Cor(X_i,T_j)  &=& \frac{b_i^j Var(T_j)}{\sqrt{Var(X_i)}\sqrt{Var(T_j)}}      \\
&=& \frac{b_i^j \lambda_j}{\sqrt{Var(X_i)}\sqrt{\lambda_j}}  = \frac{b_i^j\sqrt{\lambda_j}}{\sqrt{Var(X_i)}}
\end{eqnarray}
In particolare se $X$ è standardizzata
\begin{equation}
Cor(X_i,T_j)=b_i^j\sqrt{\lambda_j}
\end{equation}
Possiamo associare alla variabile $X_i$ le coordinate
\begin{equation}\label{eq:VarCoord}
(\sqrt{\lambda_1}b_i^1,\dots,\sqrt{\lambda_p}b_i^p)
\end{equation}
 




```{r,fig.width=5, fig.height=4,echo=FALSE, fig.cap="Variabili \\label{fig:variabili}"}
plot(x=AScR::circle()$x,y=AScR::circle()$y,type="l",asp=1,
     xlab=paste("Comp. 1","(",round(iris.pca$eig[1,2],2),"%",")"),
     ylab=paste("Comp. 2","(",round(iris.pca$eig[2,2],2),"%",")"))
abline(h = 0,lty=2)
abline(v = 0,lty=2)

var=iris.pca$var$coord

arrows(x0=0,y0=0,x1=var[1,1],y1=var[1,2],angle=10,length = 0.10)
text(x =var[1,1],y=var[1,2]+0.1,labels =row.names(var)[1],cex=0.8)

arrows(x0=0,y0=0,x1=var[2,1],y1=var[2,2],angle=10,length = 0.10)
text(x =var[2,1],y=var[2,2]+0.1,labels =row.names(var)[2],cex=0.8)

arrows(x0=0,y0=0,x1=var[3,1],y1=var[3,2],angle=10,length = 0.10)
text(x =var[3,1],y=var[3,2]+0.15,labels =row.names(var)[3],cex=0.8)

arrows(x0=0,y0=0,x1=var[4,1],y1=var[4,2],angle=10,length = 0.10)
text(x =var[4,1],y=var[4,2]-0.15,labels =row.names(var)[4],cex=0.8)

```


Un'altra importante proprietà
\begin{equation}
Cor(X_i,X_j)=b_i^1b_j^1\lambda_1+\dots+b_i^pb_j^p\lambda_p
\end{equation}
che è il prodotto scalare nel sistema di coordinate appena definito. Spiegazione su grafico.
    
Nell'oggetto *iris.pca$var* abbiamo i seguenti oggetti
```{r}
names(iris.pca$var)



iris.pca$var$coord # coordinate
P=iris.pca$svd$V # matrice loading
L=diag(iris.pca$svd$vs) # matrice diagonale radici autovalori
P%*%L # definizione coordinate
iris.pca$var$cor # coincide con le coordinate:X standardizzata
```


 
quanta parte della varianza di ogni variabile è letta dalle componenti:
misura qaulità rappresentazione
```{r}
iris.pca$var$cos2 
```


osservazioni:
1. la somma per riga =1
```{r}
rowSums(iris.pca$var$cos2 )
```


2. la somma per colonna = autovettori (varianza delle componenti)
```{r}
colSums(iris.pca$var$cos2)
```


3. è uguale a 
```{r}
iris.pca$var$cor^2
```






```{r,fig.width=8, fig.height=8,echo=FALSE, fig.cap="Cos2 variabili \\label{fig:variabili_cos2}"}
par(mfrow=c(2,2))

x=iris.pca$var$cos2[order(iris.pca$var$cos2[,1],decreasing = TRUE),]
barplot(height = x[,1],col="steelblue",cex.names = 0.75,ylim=c(0,max(x[,1])+0.1),
        main="Rispetto Comp. 1")

x=iris.pca$var$cos2[order(iris.pca$var$cos2[,2],decreasing = TRUE),]
barplot(height = x[,2],col="steelblue",cex.names = 0.75,ylim=c(0,max(x[,2])+0.1),
        main="Rispetto Comp. 2")

x=iris.pca$var$cos2[,1]+iris.pca$var$cos2[,2]
x=x[order(x,decreasing = TRUE)]
barplot(height = x,col="steelblue",cex.names = 0.75,ylim=c(0,max(x)+0.1),
        main="Rispetto Comp. 1-2")
```





```{r}
iris.pca$var$contrib # contributo della variabile nella costruzione della componente principale
sweep(x = iris.pca$var$cos2,MARGIN = 2,STATS = colSums(x =iris.pca$var$cos2 ),FUN = "/" )*100
```




```{r,fig.width=8, fig.height=8,echo=FALSE, fig.cap="Contributo variabili \\label{fig:variabili_contributo}"}
par(mfrow=c(2,2))

x=iris.pca$var$contrib[order(iris.pca$var$contrib[,1],decreasing = TRUE),]
barplot(height = x[,1],col="steelblue",cex.names = 0.75,
        main="Rispetto Comp. 1")
abline(h = 100/nrow(x),col="red",lty=2,lwd=2)

x=iris.pca$var$contrib[order(iris.pca$var$contrib[,2],decreasing = TRUE),]
barplot(height = x[,2],col="steelblue",cex.names = 0.75,
        main="Rispetto Comp. 2")
abline(h = 100/nrow(x),col="red",lty=2,lwd=2)

x=(iris.pca$var$cos2[,1]+iris.pca$var$cos2[,2])/(iris.pca$eig[1,1]+iris.pca$eig[2,1])*100
x=x[order(x,decreasing = TRUE)]
barplot(height = x,col="steelblue",cex.names = 0.75,
        main="Rispetto Comp. 1-2")
abline(h = 100/length(x),col="red",lty=2,lwd=2)


```





reference dashed line is also shown on the barplot. This reference line corresponds 
to the expected value if the contribution where uniform.



### Nota: dati non completi

### Esempio sangue

 Dati sangue aprile: ho tolto i bianchi, e le diluizioni maggiori di 8 
 le ho rese della stessa categoria
```{r}
data(Sangue,package = "Master")
dim(Sangue)
summary(Sangue[,1:2])


library(FactoMineR)
res=PCA(X = Sangue,scale.unit = FALSE,ncp = 5,quali.sup = c(1:2),graph = FALSE)
names(res)

names(res$eig)
head(round(res$eig,2))
library(factoextra)
```





```{r,fig.width=5, fig.height=4,echo=FALSE, fig.cap="Autovalori"}
p=barplot(height = res$eig[1:10,1],xlab = "Componenti Principali",
          ylim = c(0,res$eig[1,1]+0.3),
          ylab = "Percentuale varianza spiegata",names.arg = 1:length(res$eig[1:10,1]),
          col = "steelblue")
lines(x = p, res$eig[1:10,1], 
      type = "b", pch = 19, col = "red")
for (i in 1:length(res$eig[1:10,1])){
  text(x = p[i]+0.2,y = res$eig[i,1]+0.2,col="red",cex=0.7,
       labels =paste(round(res$eig[i,2],1), "%" ))
}



names(res$ind)
```





```{r, fig.width=5, fig.height=4,echo=FALSE, fig.cap="Individui per diluizione"}
plot(x = res$ind$coord[,1],y = res$ind$coord[,2],asp = 1,pch=20,
     col=c("black","red","green4","blue","brown")[Sangue$Diluizione],
     xlab=paste("Comp. 1","(",round(iris.pca$eig[1,2],2),"%",")"),
     ylab=paste("Comp. 2","(",round(iris.pca$eig[2,2],2),"%",")"))
legend("topright",legend = levels(Sangue$Diluizione),text.col=c("black","red","green4","blue","brown"),cex=0.7)
abline(h = 0,lty=2)
abline(v = 0,lty=2)
```





```{r,fig.width=5, fig.height=4,echo=FALSE, fig.cap="Individui per invecchiamento"}
plot(x = res$ind$coord[,1],y = res$ind$coord[,2],asp = 1,pch=20,
     col=c("black","red","green4")[Sangue$Invecchiamento],
     xlab=paste("Comp. 1","(",round(iris.pca$eig[1,2],2),"%",")"),
     ylab=paste("Comp. 2","(",round(iris.pca$eig[2,2],2),"%",")"))
legend("topright",legend = levels(Sangue$Invecchiamento),text.col=c("black","red","green4"),cex=0.7)
abline(h = 0,lty=2)
abline(v = 0,lty=2)


names(res$var)

plot.PCA(x = res,choix = "var")

```



```{r}
plot(res$var$contrib[,1],type="l", col="blue",ylim=c(0,7),ylab="")
par(new=TRUE)
plot(res$var$contrib[,2],type="l",col="red",ylim=c(0,7),ylab="")
legend("topleft", legend=c("Comp 1", "Comp 2"),
       col=c("blue", "red"), cex=0.8,lty=1)
```






Percentuale di varianza delle variabili letta dalle prime 2 componenti
```{r}
plot(res$var$cos2[,1]+res$var$cos2[,2],type="l")
```





